{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71512913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = Path(\"../data_final\")\n",
    "TRAIN_PATH = DATA_DIR / \"vm_train.parquet\"\n",
    "VAL_PATH   = DATA_DIR / \"vm_val.parquet\"\n",
    "TEST_PATH  = DATA_DIR / \"vm_test.parquet\"\n",
    "\n",
    "TARGET_COL = \"critical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "199b27f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split files...\n",
      "Train rows: 632426\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data Splits\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Loading split files...\")\n",
    "df_train = pl.read_parquet(TRAIN_PATH)\n",
    "df_val   = pl.read_parquet(VAL_PATH)\n",
    "df_test  = pl.read_parquet(TEST_PATH)\n",
    "\n",
    "print(f\"Train rows: {df_train.height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c98fadbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Features: 23\n"
     ]
    }
   ],
   "source": [
    "# 2. Strict Feature Selection (Added Timestamps)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# A. Identifiers & Timing (Updated with Raw Timestamps)\n",
    "feat_timing = [\n",
    "    \"day_idx\", \n",
    "    \"hour_of_day\",\n",
    "    \"ts_vm_created\",       # Added per request\n",
    "    \"ts_first_vm_created\"  # Added per request (Tenant join time)\n",
    "]\n",
    "\n",
    "# B. Static VM Config & Deployment Metadata\n",
    "feat_static = [\n",
    "    \"vm_virtual_core_count\",\n",
    "    \"vm_memory_gb\",\n",
    "    \"vm_mem_per_core\",\n",
    "    \"deployment_size\",\n",
    "    \"log_deployment_size\",\n",
    "    \"count_vms_created\",\n",
    "    \"sub_first_day\",\n",
    "    \"sub_first_hour\"\n",
    "]\n",
    "\n",
    "# C. Tenant History Features (Safe)\n",
    "feat_history = [c for c in df_train.columns if c.startswith(\"hist_\")]\n",
    "\n",
    "# D. Categorical Features\n",
    "feat_categorical = [\"vm_category\"]\n",
    "\n",
    "# Combine\n",
    "SAFE_NUMERIC_COLS = feat_timing + feat_static + feat_history\n",
    "SAFE_CATEGORICAL_COLS = feat_categorical\n",
    "ALL_SAFE_FEATURES = SAFE_NUMERIC_COLS + SAFE_CATEGORICAL_COLS\n",
    "\n",
    "print(f\"\\nTotal Features: {len(ALL_SAFE_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "255aa040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare X and y\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_X_y(df_polars):\n",
    "    X = df_polars.select(ALL_SAFE_FEATURES).to_pandas()\n",
    "    y = df_polars.select(TARGET_COL).to_pandas().values.ravel()\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = get_X_y(df_train)\n",
    "X_val, y_val     = get_X_y(df_val)\n",
    "X_test, y_test   = get_X_y(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2231ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build Robust Pipeline\n",
    "# ------------------------------------------------------------------------------\n",
    "# Numeric Pipeline: Median Imputation -> Scaling\n",
    "# Scaling is CRITICAL here because ts_vm_created is ~2,000,000 while\n",
    "# vm_virtual_core_count is ~4.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical Pipeline: Unknown Imputation -> OneHotEncoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, SAFE_NUMERIC_COLS),\n",
    "        ('cat', categorical_transformer, SAFE_CATEGORICAL_COLS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000, \n",
    "        solver='lbfgs', \n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5afdc69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Training Complete.\n",
      "\n",
      "--- Validation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90    126056\n",
      "           1       0.13      0.90      0.23      3949\n",
      "\n",
      "    accuracy                           0.82    130005\n",
      "   macro avg       0.56      0.86      0.56    130005\n",
      "weighted avg       0.97      0.82      0.87    130005\n",
      "\n",
      "Validation ROC AUC: 0.8981\n",
      "\n",
      "--- Test Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.89    128977\n",
      "           1       0.09      0.85      0.16      2872\n",
      "\n",
      "    accuracy                           0.81    131849\n",
      "   macro avg       0.54      0.83      0.53    131849\n",
      "weighted avg       0.98      0.81      0.88    131849\n",
      "\n",
      "Test ROC AUC: 0.8760\n"
     ]
    }
   ],
   "source": [
    "# 5. Train & Evaluate\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# Validation\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\n--- Validation Report ---\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Validation ROC AUC: {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
    "\n",
    "# Test\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- Test Report ---\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"Test ROC AUC: {roc_auc_score(y_test, y_test_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a7f8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Overall Features:\n",
      "                          feature  coefficient  abs_coeff\n",
      "5                    vm_memory_gb    -1.390185   1.390185\n",
      "18             hist_cpu_mean_mean     1.365360   1.365360\n",
      "6                 vm_mem_per_core     0.802334   0.802334\n",
      "15             hist_critical_frac     0.703456   0.703456\n",
      "24             vm_category_Unkown    -0.657247   0.657247\n",
      "20            hist_frac_gt60_mean    -0.472322   0.472322\n",
      "21      hist_day_night_ratio_mean    -0.433790   0.433790\n",
      "22  vm_category_Delay-insensitive    -0.395550   0.395550\n",
      "17              hist_lifetime_std     0.356563   0.356563\n",
      "19                  hist_p95_mean     0.335068   0.335068\n"
     ]
    }
   ],
   "source": [
    "# 6. Feature Importance (Check if timestamps matter)\n",
    "# ------------------------------------------------------------------------------\n",
    "cat_names = model.named_steps['preprocessor'].named_transformers_['cat']['encoder'].get_feature_names_out(SAFE_CATEGORICAL_COLS)\n",
    "final_feature_names = SAFE_NUMERIC_COLS + list(cat_names)\n",
    "coeffs = model.named_steps['classifier'].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': final_feature_names,\n",
    "    'coefficient': coeffs,\n",
    "    'abs_coeff': np.abs(coeffs)\n",
    "}).sort_values(by='abs_coeff', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Overall Features:\")\n",
    "print(coef_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
