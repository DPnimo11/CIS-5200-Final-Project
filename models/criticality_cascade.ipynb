{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49acaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Two-stage \"criticality cascade\" model (CIS 5200 final)\n",
    "# \n",
    "# Stage 1: cost-sensitive logistic regression (high recall)\n",
    "# Stage 2: random forest on \"maybe critical\" VMs (improve precision)\n",
    "# \n",
    "# Features: ONLY arrival-time columns (23) from README, excluding IDs.\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer   # <-- add this\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "RANDOM_STATE = 520\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_PATH = \"../data_final/vm_request_table_with_split.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d5f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 73\n",
      "Rows: 894280\n",
      "shape: (3, 2)\n",
      "┌───────┬────────┐\n",
      "│ split ┆ count  │\n",
      "│ ---   ┆ ---    │\n",
      "│ str   ┆ u32    │\n",
      "╞═══════╪════════╡\n",
      "│ train ┆ 632426 │\n",
      "│ test  ┆ 131849 │\n",
      "│ val   ┆ 130005 │\n",
      "└───────┴────────┘\n",
      "shape: (2, 2)\n",
      "┌──────────┬────────┐\n",
      "│ critical ┆ count  │\n",
      "│ ---      ┆ ---    │\n",
      "│ i8       ┆ u32    │\n",
      "╞══════════╪════════╡\n",
      "│ 1        ┆ 309292 │\n",
      "│ 0        ┆ 584988 │\n",
      "└──────────┴────────┘\n",
      "Train shape: (632426, 24)\n",
      "Val shape:   (130005, 24)\n",
      "Test shape:  (131849, 24)\n",
      "Positive rate (critical=1):\n",
      "train: 0.3593\n",
      "val  : 0.3029\n",
      "test : 0.3237\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Load final dataset and restrict to arrival-time features\n",
    "\n",
    "# %%\n",
    "# Load with Polars (fast)\n",
    "df_pl = pl.read_parquet(DATA_PATH)\n",
    "\n",
    "print(\"Columns:\", len(df_pl.columns))\n",
    "print(\"Rows:\", df_pl.height)\n",
    "\n",
    "# Sanity check the splits\n",
    "print(df_pl[\"split\"].value_counts())\n",
    "print(df_pl[\"critical\"].value_counts())\n",
    "\n",
    "# %%\n",
    "# Arrival-time columns we are allowed to use for an incoming VM.\n",
    "# These are exactly the non-ID features in the README's \"Features we HAVE at arrival time\":\n",
    "arrival_feature_cols = [\n",
    "    \"ts_vm_created\",\n",
    "    \"day_idx\",\n",
    "    \"hour_of_day\",\n",
    "    \"vm_category\",\n",
    "    \"vm_virtual_core_count\",\n",
    "    \"vm_memory_gb\",\n",
    "    \"vm_mem_per_core\",\n",
    "    \"deployment_size\",\n",
    "    \"log_deployment_size\",\n",
    "    \"ts_first_vm_created\",\n",
    "    \"count_vms_created\",\n",
    "    \"sub_first_day\",\n",
    "    \"sub_first_hour\",\n",
    "    \"hist_n_vms\",\n",
    "    \"hist_n_critical\",\n",
    "    \"hist_has_past\",\n",
    "    \"hist_critical_frac\",\n",
    "    \"hist_lifetime_mean\",\n",
    "    \"hist_lifetime_std\",\n",
    "    \"hist_cpu_mean_mean\",\n",
    "    \"hist_p95_mean\",\n",
    "    \"hist_frac_gt60_mean\",\n",
    "    \"hist_day_night_ratio_mean\",\n",
    "]\n",
    "\n",
    "# Sanity: make sure all of these exist in the table\n",
    "missing = [c for c in arrival_feature_cols if c not in df_pl.columns]\n",
    "assert not missing, f\"Missing expected arrival-time cols: {missing}\"\n",
    "\n",
    "# We'll drop IDs and any post-hoc columns implicitly by selecting only the list above.\n",
    "label_col = \"critical\"\n",
    "split_col = \"split\"\n",
    "\n",
    "# Convert split slices to pandas for sklearn\n",
    "train_pl = df_pl.filter(pl.col(split_col) == \"train\")\n",
    "val_pl   = df_pl.filter(pl.col(split_col) == \"val\")\n",
    "test_pl  = df_pl.filter(pl.col(split_col) == \"test\")\n",
    "\n",
    "train = train_pl.select(arrival_feature_cols + [label_col]).to_pandas()\n",
    "val   = val_pl.select(arrival_feature_cols + [label_col]).to_pandas()\n",
    "test  = test_pl.select(arrival_feature_cols + [label_col]).to_pandas()\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Val shape:  \", val.shape)\n",
    "print(\"Test shape: \", test.shape)\n",
    "\n",
    "# %%\n",
    "X_train = train[arrival_feature_cols]\n",
    "y_train = train[label_col].astype(int).values\n",
    "\n",
    "X_val = val[arrival_feature_cols]\n",
    "y_val = val[label_col].astype(int).values\n",
    "\n",
    "X_test = test[arrival_feature_cols]\n",
    "y_test = test[label_col].astype(int).values\n",
    "\n",
    "print(\"Positive rate (critical=1):\")\n",
    "for name, y in [(\"train\", y_train), (\"val\", y_val), (\"test\", y_test)]:\n",
    "    print(f\"{name:5s}: {y.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc5e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Preprocessing and metric utilities\n",
    "# \n",
    "# * One-hot encode `vm_category`\n",
    "# * Impute missing values\n",
    "# * Standard-scale numeric features\n",
    "# * Helper functions to choose thresholds and evaluate cascade\n",
    "\n",
    "# %%\n",
    "categorical_features = [\"vm_category\"]\n",
    "numeric_features = [c for c in arrival_feature_cols if c not in categorical_features]\n",
    "\n",
    "# Categorical: impute then one-hot\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Handle sklearn version differences (sparse vs sparse_output)\n",
    "try:\n",
    "    ohe = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False,  # sklearn >= 1.2\n",
    "    )\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse=False,         # older sklearn versions\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", cat_imputer),\n",
    "        (\"ohe\", ohe),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Numeric: median imputation + scaling\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# %%\n",
    "def precision_recall_f1(y_true, y_pred):\n",
    "    \"\"\"Return (precision, recall, f1) for positive class 1.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def choose_t1_for_target_recall(y_true, p_hat, target_recall=0.98):\n",
    "    \"\"\"\n",
    "    Stage-1 threshold: find t1 with recall >= target_recall,\n",
    "    and among those, choose the one with best precision.\n",
    "    If none reach target recall, choose the one with best recall (ties by precision).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    p_hat = np.asarray(p_hat)\n",
    "\n",
    "    thresholds = np.linspace(0.01, 0.8, 80)\n",
    "    best_t = 0.5\n",
    "    best_prec = -1.0\n",
    "    best_rec = -1.0\n",
    "    best_f1 = -1.0\n",
    "    best_meets_target = False\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (p_hat >= t).astype(int)\n",
    "        prec, rec, f1 = precision_recall_f1(y_true, y_pred)\n",
    "\n",
    "        meets_target = rec >= target_recall\n",
    "        if meets_target:\n",
    "            # prioritize constraints: recall >= target, then precision\n",
    "            if (not best_meets_target) or (prec > best_prec):\n",
    "                best_meets_target = True\n",
    "                best_t, best_prec, best_rec, best_f1 = t, prec, rec, f1\n",
    "        else:\n",
    "            # if target never met, fall back to max recall, then precision\n",
    "            if (not best_meets_target) and (rec > best_rec or (rec == best_rec and prec > best_prec)):\n",
    "                best_t, best_prec, best_rec, best_f1 = t, prec, rec, best_f1\n",
    "\n",
    "    return best_t, best_prec, best_rec, best_f1\n",
    "\n",
    "\n",
    "def choose_best_f1_threshold(y_true, p_hat, num_grid=80, lo=0.05, hi=0.95):\n",
    "    \"\"\"Stage-2 threshold: sweep and pick t2 that maximizes F1 for class 1.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    p_hat = np.asarray(p_hat)\n",
    "\n",
    "    thresholds = np.linspace(lo, hi, num_grid)\n",
    "    best_t = 0.5\n",
    "    best_prec = 0.0\n",
    "    best_rec = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (p_hat >= t).astype(int)\n",
    "        prec, rec, f1 = precision_recall_f1(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_prec, best_rec, best_f1 = t, prec, rec, f1\n",
    "\n",
    "    return best_t, best_prec, best_rec, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768febef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 (threshold=0.5) on VAL\n",
      "[[80196 10432]\n",
      " [15122 24255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8414    0.8849    0.8626     90628\n",
      "           1     0.6993    0.6160    0.6550     39377\n",
      "\n",
      "    accuracy                         0.8034    130005\n",
      "   macro avg     0.7703    0.7504    0.7588    130005\n",
      "weighted avg     0.7983    0.8034    0.7997    130005\n",
      "\n",
      "Chosen Stage-1 threshold t1 = 0.140\n",
      "VAL Stage-1 metrics at t1:\n",
      "[[15094 75534]\n",
      " [  688 38689]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9564    0.1665    0.2837     90628\n",
      "           1     0.3387    0.9825    0.5038     39377\n",
      "\n",
      "    accuracy                         0.4137    130005\n",
      "   macro avg     0.6476    0.5745    0.3937    130005\n",
      "weighted avg     0.7693    0.4137    0.3504    130005\n",
      "\n",
      "(prec=0.3387, rec=0.9825, f1=0.5038)\n",
      "Stage 1 on TRAIN at t1\n",
      "[[ 50484 354703]\n",
      " [  2603 224636]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9510    0.1246    0.2203    405187\n",
      "           1     0.3877    0.9885    0.5570    227239\n",
      "\n",
      "    accuracy                         0.4350    632426\n",
      "   macro avg     0.6694    0.5566    0.3887    632426\n",
      "weighted avg     0.7486    0.4350    0.3413    632426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Stage 1: Cost-sensitive logistic regression\n",
    "# \n",
    "# Goal: very high recall on criticals, accept more false positives.\n",
    "\n",
    "# %%\n",
    "stage1_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\n",
    "            \"logreg\",\n",
    "            LogisticRegression(\n",
    "                penalty=\"l2\",\n",
    "                C=1.0,\n",
    "                class_weight=\"balanced\",  # cost-sensitive for rare positives\n",
    "                solver=\"liblinear\",\n",
    "                max_iter=1000,\n",
    "                random_state=RANDOM_STATE,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stage1_clf.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Probabilities for positive class\n",
    "p_train_stage1 = stage1_clf.predict_proba(X_train)[:, 1]\n",
    "p_val_stage1   = stage1_clf.predict_proba(X_val)[:, 1]\n",
    "p_test_stage1  = stage1_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Baseline Stage-1 metrics with default threshold 0.5 (just for reference)\n",
    "y_val_stage1_05 = (p_val_stage1 >= 0.5).astype(int)\n",
    "print(\"Stage 1 (threshold=0.5) on VAL\")\n",
    "print(confusion_matrix(y_val, y_val_stage1_05))\n",
    "print(classification_report(y_val, y_val_stage1_05, digits=4))\n",
    "\n",
    "# %%\n",
    "# Choose t1 on validation data to hit high recall on criticals\n",
    "TARGET_RECALL_STAGE1 = 0.98\n",
    "\n",
    "t1, prec1, rec1, f1_1 = choose_t1_for_target_recall(\n",
    "    y_true=y_val,\n",
    "    p_hat=p_val_stage1,\n",
    "    target_recall=TARGET_RECALL_STAGE1,\n",
    ")\n",
    "\n",
    "print(f\"Chosen Stage-1 threshold t1 = {t1:.3f}\")\n",
    "print(f\"VAL Stage-1 metrics at t1:\")\n",
    "y_val_stage1 = (p_val_stage1 >= t1).astype(int)\n",
    "print(confusion_matrix(y_val, y_val_stage1))\n",
    "print(classification_report(y_val, y_val_stage1, digits=4))\n",
    "print(f\"(prec={prec1:.4f}, rec={rec1:.4f}, f1={f1_1:.4f})\")\n",
    "\n",
    "# %%\n",
    "# We'll also inspect train performance at t1 (just to see overfitting)\n",
    "y_train_stage1 = (p_train_stage1 >= t1).astype(int)\n",
    "print(\"Stage 1 on TRAIN at t1\")\n",
    "print(confusion_matrix(y_train, y_train_stage1))\n",
    "print(classification_report(y_train, y_train_stage1, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d050304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed feature shapes:\n",
      "X_train_pre: (632426, 25)\n",
      "X_val_pre:   (130005, 25)\n",
      "X_test_pre:  (131849, 25)\n",
      "Candidate fraction (maybe critical) at Stage 1:\n",
      "train: 0.9161\n",
      "val  : 0.8786\n",
      "test : 0.9177\n",
      "Stage-2 training set shape (candidates only): (579339, 25)\n",
      "Stage-2 val set shape (candidates only):      (114223, 25)\n",
      "Positive rate in Stage-2 TRAIN candidates:    0.3877453442630308\n",
      "Positive rate in Stage-2 VAL candidates:      0.3387146196475316\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Precompute preprocessed feature matrices for Stage 2\n",
    "\n",
    "# %%\n",
    "# Get fitted preprocessor from Stage-1 pipeline\n",
    "fitted_preprocess = stage1_clf.named_steps[\"preprocess\"]\n",
    "\n",
    "X_train_pre = fitted_preprocess.transform(X_train)\n",
    "X_val_pre   = fitted_preprocess.transform(X_val)\n",
    "X_test_pre  = fitted_preprocess.transform(X_test)\n",
    "\n",
    "print(\"Preprocessed feature shapes:\")\n",
    "print(\"X_train_pre:\", X_train_pre.shape)\n",
    "print(\"X_val_pre:  \", X_val_pre.shape)\n",
    "print(\"X_test_pre: \", X_test_pre.shape)\n",
    "\n",
    "# %%\n",
    "# Candidate sets: VMs that Stage 1 thinks \"might be critical\" (p >= t1)\n",
    "cand_mask_train = p_train_stage1 >= t1\n",
    "cand_mask_val   = p_val_stage1 >= t1\n",
    "cand_mask_test  = p_test_stage1 >= t1  # used only for diagnostics\n",
    "\n",
    "print(\"Candidate fraction (maybe critical) at Stage 1:\")\n",
    "for name, mask in [\n",
    "    (\"train\", cand_mask_train),\n",
    "    (\"val\", cand_mask_val),\n",
    "    (\"test\", cand_mask_test),\n",
    "]:\n",
    "    print(f\"{name:5s}: {mask.mean():.4f}\")\n",
    "\n",
    "X_train_cand_pre = X_train_pre[cand_mask_train]\n",
    "y_train_cand = y_train[cand_mask_train]\n",
    "\n",
    "X_val_cand_pre = X_val_pre[cand_mask_val]\n",
    "y_val_cand = y_val[cand_mask_val]\n",
    "\n",
    "print(\"Stage-2 training set shape (candidates only):\", X_train_cand_pre.shape)\n",
    "print(\"Stage-2 val set shape (candidates only):     \", X_val_cand_pre.shape)\n",
    "print(\"Positive rate in Stage-2 TRAIN candidates:   \", y_train_cand.mean())\n",
    "print(\"Positive rate in Stage-2 VAL candidates:     \", y_val_cand.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f521b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Stage-2 threshold t2 = 0.494\n",
      "Stage 2 metrics on VAL candidate subset at t2:\n",
      "[[64078 11456]\n",
      " [13200 25489]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8292    0.8483    0.8387     75534\n",
      "           1     0.6899    0.6588    0.6740     38689\n",
      "\n",
      "    accuracy                         0.7841    114223\n",
      "   macro avg     0.7596    0.7536    0.7563    114223\n",
      "weighted avg     0.7820    0.7841    0.7829    114223\n",
      "\n",
      "(prec=0.6899, rec=0.6588, f1=0.6740)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Stage 2: Random forest on candidate region\n",
    "# \n",
    "# Train only on VMs with p_stage1 >= t1. Focus on improving precision.\n",
    "\n",
    "# %%\n",
    "stage2_rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=0.4,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "stage2_rf.fit(X_train_cand_pre, y_train_cand)\n",
    "\n",
    "# %%\n",
    "# Evaluate Stage-2 alone on candidate VAL subset\n",
    "p_val_stage2_cand = stage2_rf.predict_proba(X_val_cand_pre)[:, 1]\n",
    "\n",
    "t2, prec2, rec2, f1_2 = choose_best_f1_threshold(\n",
    "    y_true=y_val_cand,\n",
    "    p_hat=p_val_stage2_cand,\n",
    ")\n",
    "\n",
    "print(f\"Chosen Stage-2 threshold t2 = {t2:.3f}\")\n",
    "print(\"Stage 2 metrics on VAL candidate subset at t2:\")\n",
    "y_val_stage2_cand = (p_val_stage2_cand >= t2).astype(int)\n",
    "print(confusion_matrix(y_val_cand, y_val_stage2_cand))\n",
    "print(classification_report(y_val_cand, y_val_stage2_cand, digits=4))\n",
    "print(f\"(prec={prec2:.4f}, rec={rec2:.4f}, f1={f1_2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406c5870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CASCADE model on VAL ===\n",
      "[[79172 11456]\n",
      " [13888 25489]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8508    0.8736    0.8620     90628\n",
      "           1     0.6899    0.6473    0.6679     39377\n",
      "\n",
      "    accuracy                         0.8051    130005\n",
      "   macro avg     0.7703    0.7604    0.7650    130005\n",
      "weighted avg     0.8020    0.8051    0.8032    130005\n",
      "\n",
      "VAL (critical=1): prec=0.6899, rec=0.6473, f1=0.6679\n",
      "=== CASCADE model on TEST ===\n",
      "[[77494 11679]\n",
      " [17290 25386]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8176    0.8690    0.8425     89173\n",
      "           1     0.6849    0.5949    0.6367     42676\n",
      "\n",
      "    accuracy                         0.7803    131849\n",
      "   macro avg     0.7512    0.7319    0.7396    131849\n",
      "weighted avg     0.7746    0.7803    0.7759    131849\n",
      "\n",
      "TEST (critical=1): prec=0.6849, rec=0.5949, f1=0.6367\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Final cascade: Stage 1 + Stage 2 combined\n",
    "# \n",
    "# Rule:\n",
    "#   if p1 < t1: predict 0 (non-critical)\n",
    "#   else:       predict 1 if p2 >= t2 else 0\n",
    "\n",
    "# %%\n",
    "def cascade_predict(p1, p2, t1, t2):\n",
    "    \"\"\"\n",
    "    p1: Stage-1 probs (array of shape [n])\n",
    "    p2: Stage-2 probs (array of shape [n])\n",
    "    Returns y_pred (0/1) after cascade rule.\n",
    "    \"\"\"\n",
    "    p1 = np.asarray(p1)\n",
    "    p2 = np.asarray(p2)\n",
    "    assert p1.shape == p2.shape\n",
    "\n",
    "    y_pred = np.zeros_like(p1, dtype=int)\n",
    "    cand_mask = p1 >= t1\n",
    "    y_pred[cand_mask] = (p2[cand_mask] >= t2).astype(int)\n",
    "    return y_pred\n",
    "\n",
    "# %%\n",
    "# Compute stage-2 probabilities for ALL points (we'll only use them where p1 >= t1)\n",
    "p_train_stage2_all = stage2_rf.predict_proba(X_train_pre)[:, 1]\n",
    "p_val_stage2_all   = stage2_rf.predict_proba(X_val_pre)[:, 1]\n",
    "p_test_stage2_all  = stage2_rf.predict_proba(X_test_pre)[:, 1]\n",
    "\n",
    "# %%\n",
    "# VAL evaluation\n",
    "y_val_cascade = cascade_predict(\n",
    "    p1=p_val_stage1,\n",
    "    p2=p_val_stage2_all,\n",
    "    t1=t1,\n",
    "    t2=t2,\n",
    ")\n",
    "\n",
    "print(\"=== CASCADE model on VAL ===\")\n",
    "print(confusion_matrix(y_val, y_val_cascade))\n",
    "print(classification_report(y_val, y_val_cascade, digits=4))\n",
    "prec_val, rec_val, f1_val = precision_recall_f1(y_val, y_val_cascade)\n",
    "print(f\"VAL (critical=1): prec={prec_val:.4f}, rec={rec_val:.4f}, f1={f1_val:.4f}\")\n",
    "\n",
    "# %%\n",
    "# TEST evaluation (final)\n",
    "y_test_cascade = cascade_predict(\n",
    "    p1=p_test_stage1,\n",
    "    p2=p_test_stage2_all,\n",
    "    t1=t1,\n",
    "    t2=t2,\n",
    ")\n",
    "\n",
    "print(\"=== CASCADE model on TEST ===\")\n",
    "print(confusion_matrix(y_test, y_test_cascade))\n",
    "print(classification_report(y_test, y_test_cascade, digits=4))\n",
    "prec_test, rec_test, f1_test = precision_recall_f1(y_test, y_test_cascade)\n",
    "print(f\"TEST (critical=1): prec={prec_test:.4f}, rec={rec_test:.4f}, f1={f1_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a7efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 (t1, t2) by F1 on VAL:\n",
      " 1: t1=0.010, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 2: t1=0.030, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 3: t1=0.051, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 4: t1=0.071, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 5: t1=0.092, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 6: t1=0.112, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 7: t1=0.133, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 8: t1=0.153, t2=0.462  prec=0.6466, rec=0.6867, f1=0.6661\n",
      " 9: t1=0.173, t2=0.462  prec=0.6466, rec=0.6866, f1=0.6660\n",
      "10: t1=0.194, t2=0.462  prec=0.6466, rec=0.6866, f1=0.6660\n",
      "\n",
      "Best (with recall >= 0.70):\n",
      "t1=0.296, t2=0.425  prec=0.6198, rec=0.7177, f1=0.6652\n",
      "\n",
      "Chosen thresholds from joint search:\n",
      "t1_new = 0.296, t2_new = 0.425\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Joint threshold search for cascade on VAL\n",
    "# \n",
    "# We search over (t1, t2) and pick the pair that maximizes F1 for critical=1\n",
    "# subject to a minimum recall constraint.\n",
    "\n",
    "# %%\n",
    "def search_cascade_thresholds(\n",
    "    p1_val,\n",
    "    p2_val,\n",
    "    y_val,\n",
    "    t1_grid=None,\n",
    "    t2_grid=None,\n",
    "    min_recall=0.65,\n",
    "    top_k=10,\n",
    "):\n",
    "    if t1_grid is None:\n",
    "        t1_grid = np.linspace(0.01, 0.5, 25)   # Stage 1 threshold range\n",
    "    if t2_grid is None:\n",
    "        t2_grid = np.linspace(0.05, 0.95, 25)  # Stage 2 threshold range\n",
    "\n",
    "    y_val = np.asarray(y_val)\n",
    "    p1_val = np.asarray(p1_val)\n",
    "    p2_val = np.asarray(p2_val)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_prec = 0.0\n",
    "    best_rec = 0.0\n",
    "    best_t1 = 0.5\n",
    "    best_t2 = 0.5\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for t1 in t1_grid:\n",
    "        for t2 in t2_grid:\n",
    "            y_pred = cascade_predict(p1_val, p2_val, t1, t2)\n",
    "            prec, rec, f1 = precision_recall_f1(y_val, y_pred)\n",
    "            all_results.append((t1, t2, prec, rec, f1))\n",
    "            # Enforce minimum recall for criticals\n",
    "            if rec >= min_recall and f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_prec = prec\n",
    "                best_rec = rec\n",
    "                best_t1 = t1\n",
    "                best_t2 = t2\n",
    "\n",
    "    # Sort and print top-K configs by F1 (just for inspection)\n",
    "    all_results_sorted = sorted(all_results, key=lambda x: x[4], reverse=True)\n",
    "    print(f\"Top {top_k} (t1, t2) by F1 on VAL:\")\n",
    "    for i, (t1, t2, prec, rec, f1) in enumerate(all_results_sorted[:top_k]):\n",
    "        print(\n",
    "            f\"{i+1:2d}: t1={t1:.3f}, t2={t2:.3f}  \"\n",
    "            f\"prec={prec:.4f}, rec={rec:.4f}, f1={f1:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nBest (with recall >= {:.2f}):\".format(min_recall))\n",
    "    print(\n",
    "        f\"t1={best_t1:.3f}, t2={best_t2:.3f}  \"\n",
    "        f\"prec={best_prec:.4f}, rec={best_rec:.4f}, f1={best_f1:.4f}\"\n",
    "    )\n",
    "    return best_t1, best_t2, best_prec, best_rec, best_f1\n",
    "\n",
    "# %%\n",
    "# Run the search on VAL\n",
    "t1_new, t2_new, prec_val_new, rec_val_new, f1_val_new = search_cascade_thresholds(\n",
    "    p1_val=p_val_stage1,\n",
    "    p2_val=p_val_stage2_all,\n",
    "    y_val=y_val,\n",
    "    min_recall=0.70,  # <-- adjust this up/down depending on how aggressive you want recall\n",
    ")\n",
    "\n",
    "print(\"\\nChosen thresholds from joint search:\")\n",
    "print(f\"t1_new = {t1_new:.3f}, t2_new = {t2_new:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b327522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuned CASCADE on VAL ===\n",
      "[[73290 17338]\n",
      " [11115 28262]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8683    0.8087    0.8374     90628\n",
      "           1     0.6198    0.7177    0.6652     39377\n",
      "\n",
      "    accuracy                         0.7811    130005\n",
      "   macro avg     0.7440    0.7632    0.7513    130005\n",
      "weighted avg     0.7930    0.7811    0.7853    130005\n",
      "\n",
      "VAL (critical=1): prec=0.6198, rec=0.7177, f1=0.6652\n",
      "=== Tuned CASCADE on TEST ===\n",
      "[[70965 18208]\n",
      " [13835 28841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8369    0.7958    0.8158     89173\n",
      "           1     0.6130    0.6758    0.6429     42676\n",
      "\n",
      "    accuracy                         0.7570    131849\n",
      "   macro avg     0.7249    0.7358    0.7293    131849\n",
      "weighted avg     0.7644    0.7570    0.7598    131849\n",
      "\n",
      "TEST (critical=1): prec=0.6130, rec=0.6758, f1=0.6429\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Re-evaluate cascade with tuned thresholds\n",
    "\n",
    "# %%\n",
    "# VAL\n",
    "y_val_cascade_tuned = cascade_predict(\n",
    "    p1=p_val_stage1,\n",
    "    p2=p_val_stage2_all,\n",
    "    t1=t1_new,\n",
    "    t2=t2_new,\n",
    ")\n",
    "\n",
    "print(\"=== Tuned CASCADE on VAL ===\")\n",
    "print(confusion_matrix(y_val, y_val_cascade_tuned))\n",
    "print(classification_report(y_val, y_val_cascade_tuned, digits=4))\n",
    "prec_val_tuned, rec_val_tuned, f1_val_tuned = precision_recall_f1(y_val, y_val_cascade_tuned)\n",
    "print(f\"VAL (critical=1): prec={prec_val_tuned:.4f}, rec={rec_val_tuned:.4f}, f1={f1_val_tuned:.4f}\")\n",
    "\n",
    "# %%\n",
    "# TEST\n",
    "y_test_cascade_tuned = cascade_predict(\n",
    "    p1=p_test_stage1,\n",
    "    p2=p_test_stage2_all,\n",
    "    t1=t1_new,\n",
    "    t2=t2_new,\n",
    ")\n",
    "\n",
    "print(\"=== Tuned CASCADE on TEST ===\")\n",
    "print(confusion_matrix(y_test, y_test_cascade_tuned))\n",
    "print(classification_report(y_test, y_test_cascade_tuned, digits=4))\n",
    "prec_test_tuned, rec_test_tuned, f1_test_tuned = precision_recall_f1(y_test, y_test_cascade_tuned)\n",
    "print(f\"TEST (critical=1): prec={prec_test_tuned:.4f}, rec={rec_test_tuned:.4f}, f1={f1_test_tuned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f796745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage-1-only (logistic) on TEST at t1 ===\n",
      "[[10290 78883]\n",
      " [  558 42118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9486    0.1154    0.2058     89173\n",
      "           1     0.3481    0.9869    0.5146     42676\n",
      "\n",
      "    accuracy                         0.3975    131849\n",
      "   macro avg     0.6483    0.5512    0.3602    131849\n",
      "weighted avg     0.7542    0.3975    0.3057    131849\n",
      "\n",
      "Stage-1-only TEST (critical=1): prec=0.3481, rec=0.9869, f1=0.5146\n",
      "\n",
      "=== Cascade vs Stage-1-only (critical=1) ===\n",
      "Stage 1 only: prec=0.3481, rec=0.9869, f1=0.5146\n",
      "Cascade     : prec=0.6849, rec=0.5949, f1=0.6367\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Optional: Compare cascade vs Stage-1-only at threshold t1\n",
    "\n",
    "# %%\n",
    "y_test_stage1_only = (p_test_stage1 >= t1).astype(int)\n",
    "\n",
    "print(\"=== Stage-1-only (logistic) on TEST at t1 ===\")\n",
    "print(confusion_matrix(y_test, y_test_stage1_only))\n",
    "print(classification_report(y_test, y_test_stage1_only, digits=4))\n",
    "\n",
    "prec1_test, rec1_test, f1_1_test = precision_recall_f1(y_test, y_test_stage1_only)\n",
    "print(f\"Stage-1-only TEST (critical=1): prec={prec1_test:.4f}, rec={rec1_test:.4f}, f1={f1_1_test:.4f}\")\n",
    "\n",
    "print(\"\\n=== Cascade vs Stage-1-only (critical=1) ===\")\n",
    "print(f\"Stage 1 only: prec={prec1_test:.4f}, rec={rec1_test:.4f}, f1={f1_1_test:.4f}\")\n",
    "print(f\"Cascade     : prec={prec_test:.4f}, rec={rec_test:.4f}, f1={f1_test:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
