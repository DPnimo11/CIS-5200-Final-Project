{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CatBoost model for VM criticality\n",
        "\n",
        "Using only arrival-time features and the time-based train/val/test splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    confusion_matrix,\n",
        ")\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = Path(\"../data_final\")\n",
        "TRAIN_PATH = DATA_DIR / \"vm_train.parquet\"\n",
        "VAL_PATH   = DATA_DIR / \"vm_val.parquet\"\n",
        "TEST_PATH  = DATA_DIR / \"vm_test.parquet\"\n",
        "\n",
        "TARGET_COL = \"critical\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading split files...\n",
            "Train rows: 632426\n",
            "Val rows:   130005\n",
            "Test rows:  131849\n"
          ]
        }
      ],
      "source": [
        "# 1) Load Data Splits\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Loading split files...\")\n",
        "df_train = pl.read_parquet(TRAIN_PATH)\n",
        "df_val   = pl.read_parquet(VAL_PATH)\n",
        "df_test  = pl.read_parquet(TEST_PATH)\n",
        "\n",
        "print(f\"Train rows: {df_train.height}\")\n",
        "print(f\"Val rows:   {df_val.height}\")\n",
        "print(f\"Test rows:  {df_test.height}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Feature Check ---\n",
            "Timing: 4\n",
            "Static: 8\n",
            "History: 10\n",
            "Categorical: 1\n",
            "Total: 23\n"
          ]
        }
      ],
      "source": [
        "# 2) Strict Feature Selection (23 arrival-time features)\n",
        "# -----------------------------------------------------------------------------\n",
        "feat_timing = [\n",
        "    \"day_idx\",\n",
        "    \"hour_of_day\",\n",
        "    \"ts_vm_created\",\n",
        "    \"ts_first_vm_created\",\n",
        "]\n",
        "\n",
        "feat_static = [\n",
        "    \"vm_virtual_core_count\",\n",
        "    \"vm_memory_gb\",\n",
        "    \"vm_mem_per_core\",\n",
        "    \"deployment_size\",\n",
        "    \"log_deployment_size\",\n",
        "    \"count_vms_created\",\n",
        "    \"sub_first_day\",\n",
        "    \"sub_first_hour\",\n",
        "]\n",
        "\n",
        "feat_history = [\n",
        "    \"hist_n_vms\",\n",
        "    \"hist_n_critical\",\n",
        "    \"hist_has_past\",\n",
        "    \"hist_critical_frac\",\n",
        "    \"hist_lifetime_mean\",\n",
        "    \"hist_lifetime_std\",\n",
        "    \"hist_cpu_mean_mean\",\n",
        "    \"hist_p95_mean\",\n",
        "    \"hist_frac_gt60_mean\",\n",
        "    \"hist_day_night_ratio_mean\",\n",
        "]\n",
        "\n",
        "feat_categorical = [\"vm_category\"]\n",
        "\n",
        "ALL_FEATURES = feat_timing + feat_static + feat_history + feat_categorical\n",
        "\n",
        "print(\"\\n--- Feature Check ---\")\n",
        "print(f\"Timing: {len(feat_timing)}\")\n",
        "print(f\"Static: {len(feat_static)}\")\n",
        "print(f\"History: {len(feat_history)}\")\n",
        "print(f\"Categorical: {len(feat_categorical)}\")\n",
        "print(f\"Total: {len(ALL_FEATURES)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train positive rate: 35.93% (count=227239)\n",
            "Class weights: [1.0, np.float64(1.7830874101716694)]\n"
          ]
        }
      ],
      "source": [
        "# 3) Helpers to prepare CatBoost inputs\n",
        "# -----------------------------------------------------------------------------\n",
        "def prepare_df(df_polars):\n",
        "    # Select features + target and convert to pandas\n",
        "    df_pd = df_polars.select(ALL_FEATURES + [TARGET_COL]).to_pandas()\n",
        "    X = df_pd[ALL_FEATURES].copy()\n",
        "    y = df_pd[TARGET_COL].values.ravel()\n",
        "    # Ensure categorical dtype\n",
        "    for col in feat_categorical:\n",
        "        X[col] = X[col].astype(\"category\")\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X_train, y_train = prepare_df(df_train)\n",
        "X_val, y_val     = prepare_df(df_val)\n",
        "X_test, y_test   = prepare_df(df_test)\n",
        "\n",
        "# Indices for CatBoost categorical features\n",
        "cat_indices = [ALL_FEATURES.index(c) for c in feat_categorical]\n",
        "\n",
        "# Class weights (neg/pos)\n",
        "neg_count = np.sum(y_train == 0)\n",
        "pos_count = np.sum(y_train == 1)\n",
        "class_weights = [1.0, neg_count / pos_count]\n",
        "\n",
        "print(f\"Train positive rate: {y_train.mean():.2%} (count={pos_count})\")\n",
        "print(f\"Class weights: {class_weights}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.8285065\ttest: 0.8165690\tbest: 0.8165690 (0)\ttotal: 188ms\tremaining: 4m 42s\n",
            "200:\tlearn: 0.8671036\ttest: 0.8358504\tbest: 0.8363609 (166)\ttotal: 28.2s\tremaining: 3m 2s\n",
            "Stopped by overfitting detector  (80 iterations wait)\n",
            "\n",
            "bestTest = 0.8363608646\n",
            "bestIteration = 166\n",
            "\n",
            "Shrink model to first 167 iterations.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7fd66d6ec590>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4) Train CatBoost with early stopping (optimize PR-friendly behaviour)\n",
        "# -----------------------------------------------------------------------------\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_indices)\n",
        "val_pool   = Pool(X_val, y_val, cat_features=cat_indices)\n",
        "\n",
        "params = dict(\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"PRAUC\",   # focus on PR AUC\n",
        "    learning_rate=0.05,\n",
        "    depth=8,\n",
        "    l2_leaf_reg=5,\n",
        "    iterations=1500,\n",
        "    random_seed=42,\n",
        "    od_type=\"Iter\",\n",
        "    od_wait=80,\n",
        "    use_best_model=True,\n",
        "    subsample=0.9,\n",
        "    colsample_bylevel=0.8,\n",
        "    class_weights=class_weights,\n",
        "    verbose=200,\n",
        ")\n",
        "\n",
        "clf = CatBoostClassifier(**params)\n",
        "clf.fit(train_pool, eval_set=val_pool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best val threshold (by F1): 0.5733 | F1=0.6713\n"
          ]
        }
      ],
      "source": [
        "# 5) Threshold search (maximize F1 on val) and metric helper\n",
        "# -----------------------------------------------------------------------------\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    split: str\n",
        "    roc_auc: float\n",
        "    pr_auc: float\n",
        "    f1: float\n",
        "    threshold: float\n",
        "    report: str\n",
        "    confusion: np.ndarray\n",
        "\n",
        "\n",
        "def pick_threshold(y_true, y_prob):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-9)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    return thresholds[best_idx], f1_scores[best_idx]\n",
        "\n",
        "\n",
        "def evaluate_split(y_true, y_prob, split_name, threshold):\n",
        "    preds = (y_prob >= threshold).astype(int)\n",
        "    return EvalResult(\n",
        "        split=split_name,\n",
        "        roc_auc=roc_auc_score(y_true, y_prob),\n",
        "        pr_auc=average_precision_score(y_true, y_prob),\n",
        "        f1=classification_report(y_true, preds, output_dict=True)[\"weighted avg\"][\"f1-score\"],\n",
        "        threshold=threshold,\n",
        "        report=classification_report(y_true, preds),\n",
        "        confusion=confusion_matrix(y_true, preds),\n",
        "    )\n",
        "\n",
        "\n",
        "# Choose threshold on val\n",
        "val_prob = clf.predict_proba(X_val)[:, 1]\n",
        "best_thr, best_f1 = pick_threshold(y_val, val_prob)\n",
        "print(f\"Best val threshold (by F1): {best_thr:.4f} | F1={best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- VAL ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86     90628\n",
            "           1       0.68      0.66      0.67     39377\n",
            "\n",
            "    accuracy                           0.80    130005\n",
            "   macro avg       0.77      0.76      0.77    130005\n",
            "weighted avg       0.80      0.80      0.80    130005\n",
            "\n",
            "ROC AUC: 0.8477\n",
            "PR  AUC: 0.7632\n",
            "F1 (weighted): 0.8031\n",
            "Confusion matrix:\n",
            " [[78506 12122]\n",
            " [13360 26017]]\n",
            "\n",
            "--- TEST ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83     89173\n",
            "           1       0.65      0.63      0.64     42676\n",
            "\n",
            "    accuracy                           0.77    131849\n",
            "   macro avg       0.74      0.73      0.74    131849\n",
            "weighted avg       0.77      0.77      0.77    131849\n",
            "\n",
            "ROC AUC: 0.8203\n",
            "PR  AUC: 0.7363\n",
            "F1 (weighted): 0.7710\n",
            "Confusion matrix:\n",
            " [[75072 14101]\n",
            " [15920 26756]]\n"
          ]
        }
      ],
      "source": [
        "# 6) Full evaluation on val and test\n",
        "# -----------------------------------------------------------------------------\n",
        "val_metrics = evaluate_split(y_val, val_prob, \"val\", best_thr)\n",
        "test_prob = clf.predict_proba(X_test)[:, 1]\n",
        "test_metrics = evaluate_split(y_test, test_prob, \"test\", best_thr)\n",
        "\n",
        "for m in [val_metrics, test_metrics]:\n",
        "    print(f\"\\n--- {m.split.upper()} ---\")\n",
        "    print(m.report)\n",
        "    print(f\"ROC AUC: {m.roc_auc:.4f}\")\n",
        "    print(f\"PR  AUC: {m.pr_auc:.4f}\")\n",
        "    print(f\"F1 (weighted): {m.f1:.4f}\")\n",
        "    print(\"Confusion matrix:\\n\", m.confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to artifacts/catboost_model.cbm\n",
            "Saved metrics to artifacts/catboost_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# 7) Save artifacts\n",
        "# -----------------------------------------------------------------------------\n",
        "ARTIFACT_DIR = Path(\"artifacts\")\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = ARTIFACT_DIR / \"catboost_model.cbm\"\n",
        "metrics_path = ARTIFACT_DIR / \"catboost_metrics.json\"\n",
        "\n",
        "clf.save_model(model_path)\n",
        "\n",
        "import json\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"val\": {\n",
        "                \"roc_auc\": val_metrics.roc_auc,\n",
        "                \"pr_auc\": val_metrics.pr_auc,\n",
        "                \"f1_weighted\": val_metrics.f1,\n",
        "                \"threshold\": val_metrics.threshold,\n",
        "            },\n",
        "            \"test\": {\n",
        "                \"roc_auc\": test_metrics.roc_auc,\n",
        "                \"pr_auc\": test_metrics.pr_auc,\n",
        "                \"f1_weighted\": test_metrics.f1,\n",
        "                \"threshold\": test_metrics.threshold,\n",
        "            },\n",
        "        },\n",
        "        f,\n",
        "        indent=2,\n",
        "    )\n",
        "\n",
        "print(f\"Saved model to {model_path}\")\n",
        "print(f\"Saved metrics to {metrics_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pymain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
