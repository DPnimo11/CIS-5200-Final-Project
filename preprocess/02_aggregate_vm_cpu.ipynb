{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89d9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DATA_DIR     = C:\\Users\\Layton\\Desktop\\cis-520\\preprocess\\data_raw\n",
      "INTERMEDIATE_DIR = C:\\Users\\Layton\\Desktop\\cis-520\\preprocess\\data_intermediate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Layton\\.conda\\envs\\SLH\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---- keep these consistent with 00/01 ----\n",
    "RAW_DATA_DIR = Path(\"data_raw\")  # TODO: same as in 00/01\n",
    "INTERMEDIATE_DIR = Path(\"data_intermediate\")\n",
    "INTERMEDIATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SECONDS_PER_HOUR = 60 * 60\n",
    "SECONDS_PER_DAY = 24 * SECONDS_PER_HOUR\n",
    "\n",
    "print(\"RAW_DATA_DIR     =\", RAW_DATA_DIR.resolve())\n",
    "print(\"INTERMEDIATE_DIR =\", INTERMEDIATE_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575b4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_hour_expr(col: str | pl.Expr) -> pl.Expr:\n",
    "    col_expr = pl.col(col) if isinstance(col, str) else col\n",
    "    return ((col_expr % SECONDS_PER_DAY) // SECONDS_PER_HOUR).cast(pl.Int32)\n",
    "\n",
    "\n",
    "cpu_schema = {\n",
    "    \"timestamp\": pl.Int64,\n",
    "    \"vm_id\": pl.Utf8,\n",
    "    \"min_cpu\": pl.Float64,\n",
    "    \"max_cpu\": pl.Float64,\n",
    "    \"avg_cpu\": pl.Float64,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089439b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 cpu shards under data_raw\\vm_cpu\n",
      "Partial aggregates will go to: C:\\Users\\Layton\\Desktop\\cis-520\\preprocess\\data_intermediate\\cpu_parts\n"
     ]
    }
   ],
   "source": [
    "CPU_DIR = RAW_DATA_DIR / \"vm_cpu\"\n",
    "\n",
    "shard_paths = sorted(glob(str(CPU_DIR / \"*.csv.gz\")))\n",
    "print(f\"Found {len(shard_paths)} cpu shards under {CPU_DIR}\")\n",
    "\n",
    "cpu_part_dir = INTERMEDIATE_DIR / \"cpu_parts\"\n",
    "cpu_part_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Partial aggregates will go to:\", cpu_part_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c792bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cpu_shard(csv_path: str | Path, out_parquet: str | Path) -> None:\n",
    "    \"\"\"\n",
    "    Aggregate one vm_cpu_readings shard into per-VM partial stats and save as Parquet.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = str(csv_path)\n",
    "    out_parquet = str(out_parquet)\n",
    "\n",
    "    # Lazy scan for streaming-friendly groupby\n",
    "    lf = pl.scan_csv(\n",
    "        csv_path,\n",
    "        has_header=True,\n",
    "        schema=cpu_schema,\n",
    "    )\n",
    "\n",
    "    lf = lf.with_columns(\n",
    "        sec_to_hour_expr(\"timestamp\").alias(\"hour_of_day\")\n",
    "    )\n",
    "\n",
    "    day_hours = list(range(8, 20))  # 08:00 - 19:59\n",
    "    night_hours = list(range(0, 8)) + list(range(20, 24))\n",
    "\n",
    "    agg_exprs = []\n",
    "\n",
    "    # basic counts/sums\n",
    "    agg_exprs.extend(\n",
    "        [\n",
    "            pl.count().alias(\"n_readings\"),\n",
    "            pl.col(\"avg_cpu\").sum().alias(\"sum_avg\"),\n",
    "            (pl.col(\"avg_cpu\") ** 2).sum().alias(\"sum_avg_sq\"),\n",
    "            pl.col(\"max_cpu\").max().alias(\"max_cpu\"),\n",
    "            (pl.col(\"avg_cpu\") > 60.0).sum().alias(\"cnt_gt_60\"),\n",
    "            (pl.col(\"avg_cpu\") > 80.0).sum().alias(\"cnt_gt_80\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # day/night aggregates\n",
    "    agg_exprs.extend(\n",
    "        [\n",
    "            pl.col(\"avg_cpu\")\n",
    "            .filter(pl.col(\"hour_of_day\").is_in(day_hours))\n",
    "            .sum()\n",
    "            .alias(\"sum_day\"),\n",
    "            pl.when(pl.col(\"hour_of_day\").is_in(day_hours))\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .sum()\n",
    "            .alias(\"cnt_day\"),\n",
    "            pl.col(\"avg_cpu\")\n",
    "            .filter(pl.col(\"hour_of_day\").is_in(night_hours))\n",
    "            .sum()\n",
    "            .alias(\"sum_night\"),\n",
    "            pl.when(pl.col(\"hour_of_day\").is_in(night_hours))\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .sum()\n",
    "            .alias(\"cnt_night\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # per-hour aggregates (0..23)\n",
    "    for h in range(24):\n",
    "        agg_exprs.append(\n",
    "            pl.col(\"avg_cpu\")\n",
    "            .filter(pl.col(\"hour_of_day\") == h)\n",
    "            .sum()\n",
    "            .alias(f\"sum_hour_{h}\")\n",
    "        )\n",
    "    for h in range(24):\n",
    "        agg_exprs.append(\n",
    "            pl.when(pl.col(\"hour_of_day\") == h)\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .sum()\n",
    "            .alias(f\"cnt_hour_{h}\")\n",
    "        )\n",
    "\n",
    "    grouped = lf.group_by(\"vm_id\").agg(agg_exprs)\n",
    "\n",
    "    # streaming=True is nice if your Polars version supports it\n",
    "    df = grouped.collect(streaming=True)\n",
    "\n",
    "    df.write_parquet(out_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a86ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CPU shards → partial per-VM stats:   0%|          | 0/125 [00:00<?, ?it/s, cpu_part_000.parquet]C:\\Users\\Layton\\AppData\\Local\\Temp\\ipykernel_10244\\262256200.py:28: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"n_readings\"),\n",
      "C:\\Users\\Layton\\AppData\\Local\\Temp\\ipykernel_10244\\262256200.py:81: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  df = grouped.collect(streaming=True)\n",
      "CPU shards → partial per-VM stats: 100%|██████████| 125/125 [15:14<00:00,  7.32s/it, cpu_part_124.parquet]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 complete: partial aggregates written to data_intermediate\\cpu_parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(shard_paths, desc=\"CPU shards → partial per-VM stats\")\n",
    "\n",
    "for i, path in enumerate(pbar):\n",
    "    out_path = cpu_part_dir / f\"cpu_part_{i:03d}.parquet\"\n",
    "\n",
    "    # Allow resuming: skip if already done\n",
    "    if out_path.exists():\n",
    "        pbar.set_postfix_str(\"skip (exists)\")\n",
    "        continue\n",
    "\n",
    "    pbar.set_postfix_str(out_path.name)\n",
    "    aggregate_cpu_shard(path, out_path)\n",
    "\n",
    "print(\"Pass 1 complete: partial aggregates written to\", cpu_part_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6423cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining 125 partial files...\n",
      "Raw combined usage rows: 2013767\n",
      "Columns: ['vm_id', 'n_readings', 'sum_avg', 'sum_avg_sq', 'cnt_gt_60', 'cnt_gt_80', 'sum_day', 'cnt_day', 'sum_night', 'cnt_night'] ...\n"
     ]
    }
   ],
   "source": [
    "part_paths = sorted(glob(str(cpu_part_dir / \"cpu_part_*.parquet\")))\n",
    "print(f\"Combining {len(part_paths)} partial files...\")\n",
    "\n",
    "# We'll use lazy scan over all partials\n",
    "lf_parts = pl.scan_parquet(str(cpu_part_dir / \"cpu_part_*.parquet\"))\n",
    "\n",
    "# Build aggregation expressions programmatically\n",
    "sum_cols = [\n",
    "    \"n_readings\",\n",
    "    \"sum_avg\",\n",
    "    \"sum_avg_sq\",\n",
    "    \"cnt_gt_60\",\n",
    "    \"cnt_gt_80\",\n",
    "    \"sum_day\",\n",
    "    \"cnt_day\",\n",
    "    \"sum_night\",\n",
    "    \"cnt_night\",\n",
    "] + [f\"sum_hour_{h}\" for h in range(24)] + [f\"cnt_hour_{h}\" for h in range(24)]\n",
    "\n",
    "agg_exprs_final = []\n",
    "\n",
    "for c in sum_cols:\n",
    "    agg_exprs_final.append(pl.col(c).sum().alias(c))\n",
    "\n",
    "# max_cpu uses max, not sum\n",
    "agg_exprs_final.append(pl.col(\"max_cpu\").max().alias(\"max_cpu\"))\n",
    "\n",
    "lf_agg = lf_parts.group_by(\"vm_id\").agg(agg_exprs_final)\n",
    "\n",
    "# Materialize\n",
    "vm_usage_raw = lf_agg.collect()\n",
    "print(\"Raw combined usage rows:\", vm_usage_raw.height)\n",
    "print(\"Columns:\", vm_usage_raw.columns[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093ca298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example rows with new features:\n",
      "shape: (5, 12)\n",
      "┌───────────┬───────────┬───────────┬──────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ vm_id     ┆ n_reading ┆ cpu_mean  ┆ cpu_std  ┆ … ┆ cpu_hour_ ┆ cpu_hour_ ┆ cpu_hour_ ┆ cpu_hour_ │\n",
      "│ ---       ┆ s         ┆ ---       ┆ ---      ┆   ┆ 0_mean    ┆ 1_mean    ┆ 2_mean    ┆ 3_mean    │\n",
      "│ str       ┆ ---       ┆ f64       ┆ f64      ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│           ┆ u32       ┆           ┆          ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
      "╞═══════════╪═══════════╪═══════════╪══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ iDRcL2N6S ┆ 1         ┆ 0.154114  ┆ 0.0      ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ V3Hte50J0 ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ wA9PsBXw2 ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ 3+4…      ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ d8YTWmfMd ┆ 14        ┆ 4.059619  ┆ 8.593959 ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ Xa1Sy8m5m ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ kFt4r1/3I ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ MIh…      ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ LxNAQGjj7 ┆ 3367      ┆ 15.668299 ┆ 7.981299 ┆ … ┆ 14.075835 ┆ 14.06538  ┆ 14.166552 ┆ 18.048023 │\n",
      "│ l8oXW+wNG ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ mqnWvu6UR ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ kJ8…      ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ OJn9EujPG ┆ 2         ┆ 21.195248 ┆ 1.462894 ┆ … ┆ 21.195248 ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ ym8iy6Pfp ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ K3NrV5wm3 ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ kdu…      ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ OzMLaUlLI ┆ 1         ┆ 0.126938  ┆ 0.0      ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
      "│ /j7ZoHpRc ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ O6+YP/hnw ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "│ GzN…      ┆           ┆           ┆          ┆   ┆           ┆           ┆           ┆           │\n",
      "└───────────┴───────────┴───────────┴──────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Avoid division by zero by using when/otherwise\n",
    "df = vm_usage_raw\n",
    "\n",
    "df = df.with_columns(\n",
    "    [\n",
    "        (pl.col(\"sum_avg\") / pl.col(\"n_readings\")).alias(\"cpu_mean\"),\n",
    "        (\n",
    "            (\n",
    "                pl.col(\"sum_avg_sq\") / pl.col(\"n_readings\")\n",
    "                - (pl.col(\"sum_avg\") / pl.col(\"n_readings\")) ** 2\n",
    "            )\n",
    "            .clip(lower_bound=0.0)\n",
    "            .sqrt()\n",
    "        ).alias(\"cpu_std\"),\n",
    "        (pl.col(\"cnt_gt_60\") / pl.col(\"n_readings\")).alias(\"cpu_frac_gt_60\"),\n",
    "        (pl.col(\"cnt_gt_80\") / pl.col(\"n_readings\")).alias(\"cpu_frac_gt_80\"),\n",
    "        pl.when(pl.col(\"cnt_day\") > 0)\n",
    "        .then(pl.col(\"sum_day\") / pl.col(\"cnt_day\"))\n",
    "        .otherwise(0.0)\n",
    "        .alias(\"day_cpu_mean\"),\n",
    "        pl.when(pl.col(\"cnt_night\") > 0)\n",
    "        .then(pl.col(\"sum_night\") / pl.col(\"cnt_night\"))\n",
    "        .otherwise(0.0)\n",
    "        .alias(\"night_cpu_mean\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"day_cpu_mean\") / (pl.col(\"night_cpu_mean\") + 1e-3)).alias(\n",
    "        \"day_night_ratio\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Per-hour mean features\n",
    "hour_mean_cols = []\n",
    "for h in range(24):\n",
    "    sum_col = f\"sum_hour_{h}\"\n",
    "    cnt_col = f\"cnt_hour_{h}\"\n",
    "    mean_col = f\"cpu_hour_{h}_mean\"\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(cnt_col) > 0)\n",
    "        .then(pl.col(sum_col) / pl.col(cnt_col))\n",
    "        .otherwise(0.0)\n",
    "        .alias(mean_col)\n",
    "    )\n",
    "    hour_mean_cols.append(mean_col)\n",
    "\n",
    "print(\"Example rows with new features:\")\n",
    "print(df.select(\n",
    "    [\"vm_id\", \"n_readings\", \"cpu_mean\", \"cpu_std\", \"cpu_frac_gt_60\",\n",
    "     \"day_cpu_mean\", \"night_cpu_mean\", \"day_night_ratio\"] + hour_mean_cols[:4]\n",
    ").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea93686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final per-VM usage table to: data_intermediate\\vm_usage_agg.parquet\n"
     ]
    }
   ],
   "source": [
    "usage_out_path = INTERMEDIATE_DIR / \"vm_usage_agg.parquet\"\n",
    "df.write_parquet(usage_out_path)\n",
    "print(\"Saved final per-VM usage table to:\", usage_out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
